---
title: | 
  | Data analysis report - 
  | The effect of an exercise intervention program on bone health after bariatric surgery: A Randomized Controlled Trial
author: "Lucas Veras"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    number_sections: true
    toc_float: 
      collapsed: true
    highlight: tango
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(eval = FALSE)
```
 
```{r loads, eval=TRUE, echo=FALSE, include=FALSE}
library(here)
library(tidyverse)
library(lme4)
library(lmerTest)
library(piecewiseSEM)
library(emmeans)
```

<br>
 
This report covers the content of a [github repository](https://github.com/verasls/BaSEIB_exercise_bone_health) that encompasses the data analysis of a manuscript entitled "The effect of an exercise intervention program on bone health after bariatric surgery: BaSEIB clinical trial". The [code](https://github.com/verasls/BaSEIB_exercise_bone_health/tree/master/code) folder contains two subfolders: [functions](https://github.com/verasls/BaSEIB_exercise_bone_health/tree/master/code/functions), with some function definitions, and [scripts](https://github.com/verasls/BaSEIB_exercise_bone_health/tree/master/code/scripts), with 8 different scripts regarding distinct steps of the data processing and statistical analysis. Also, the .Rmd file used to generate this report is inside the [code](https://github.com/verasls/BaSEIB_exercise_bone_health/tree/master/code) folder and the code to generate the manuscript figures are inside the [figs](https://github.com/verasls/BaSEIB_exercise_bone_health/tree/master/figs) folder.

<br>

# Load and tidy data

The [01_tidy_data.R](https://github.com/verasls/BaSEIB_exercise_bone_health/blob/master/code/scripts/01_tidy_data.R) script loads the database.csv file. It, then, selects, renames and recodes some variables and excludes data from subjects who do not have at least one follow-up assessment.

<br>

# Baseline characteristics

The next step was to compute descriptive statistics for the demographic, anthropometric and clinical variables at baseline and also to see if there were any differences in those variables between the subjects included in the final analyses and the dropouts, which was done in the [02_explore_data.R](https://github.com/verasls/BaSEIB_exercise_bone_health/blob/master/code/scripts/02_explore_data.R) script. 

To do so, new data frames needed to be built, with one group per data frame containing only the baseline values of the variables.

```{r compare_df}
baseline_df <- read_data(here("data", "baseline_df.csv"))
control_df <- filter(baseline_df, group == "Control" & exclude == "No")
exercise_df <- filter(baseline_df, group == "Exercise" & exclude == "No")
exercise_under_50_df <- filter(baseline_df, attend_cat == "Under 50% training attendance" & exclude == "No")
exercise_over_50_df <- filter(baseline_df, attend_cat == "Over 50% training attendance" & exclude == "No")
baseline_inc_df <- filter(baseline_df, exclude == "No")
baseline_exc_df <- filter(baseline_df, exclude == "Yes")
```

Regarding the test for differences between the included subjects and the dropouts, for the continuous variables, first, normality tests were run with the `shapiro.test()` function. Then, independent samples t tests were run with the `t.test()` function. Both functions are on the `stats` package. For the categorical variabels, the chi-square test was used to the baseline comparisons with the `CrossTable()` function of the `gmodels` [package](https://cran.r-project.org/web/packages/gmodels/gmodels.pdf).

An example of the code used to these analyses is shown below.

```{r baseline_comparisons}
# Compare: control X exercise ---------------------------------------------
# Normality test
shapiro.test(baseline_inc_df$BMI)
shapiro.test(baseline_exc_df$BMI)
# Independent samples t test
t.test(BMI ~ exclude, data = baseline_df, paired = FALSE)
# Chi-square test
CrossTable(
  baseline_df$sex, baseline_df$exclude, 
  fisher = TRUE, chisq = TRUE, format = "SPSS"
)
```

<br>

# Primary analysis

A primary analysis was conducted comparing the outcomes between the groups (control and exercise) as randomized, using an intention-to-treat approach. First, the treatment effect on bone parameters was tested. These analyses were separated into two scripts, [03_pri_anal_model_BMD_vars.R](https://github.com/verasls/BaSEIB_exercise_bone_health/blob/master/code/scripts/03_pri_anal_model_BMD_vars.R) for the bone mineral density (BMD) variables, and [04_pri_anal_model_bioch_BMSi_vars.R](https://github.com/verasls/BaSEIB_exercise_bone_health/blob/master/code/scripts/04_pri_anal_model_bioch_BMSi_vars.R) for the biochemical variables and the bone material strength index (BMSi). The analyses in both scripts were done similarly, varying only in the variables.

Initially, to prepare for the statistical analyses, a sum contrast was set to the `group` variable and a polynomial contrast was set to the `time` variable. This was accomplished with the functions `contrasts()`, `contr.sum()` and `contr.poly()` from the `stats` package. After that, a different data frame was created for each variable analyzed, selecting only the necessary columns from the main data frame. These variables were:

- `subj`: subject identifier;
- `group` and `time`: factors included in the analysis;
- Dependent variable (e.g., `FN_BMD`);
- Covariates: Baseline values of the dependent variable (e.g., `FN_BMD_adjust`), age and BMI, as well as categorical variables such as surgery type, sex, menopause, thiazide diuretics use and smoking.

An example of the code used to this process is shown below:

```{r setup_LMM}
# Set contrasts of variable group to sum
contrasts(df$group) <- matrix(rev(contr.sum(2)), ncol = 1)
# Set contrasts of variable time to polynomial
contrasts(df$time) <- contr.poly(4)

# Select variables
LS_data <- df %>% 
  dplyr::select(
    subj, time, group, LS_BMD, LS_BMD_adjust, BMI_adjust,
    surgery, age, menopause, diabetes, thiazides, smoker
  )
```

## Linear mixed models

The treatment effect on bone parameters was tested by linear mixed-models analysis  using the `lmer()` function of the `lme4` [package](https://cran.r-project.org/web/packages/lme4/lme4.pdf). The models included group, time and group X time interaction as fixed factors, the covariates mentioned above, and the subjects as a random factor. An example of the code to build the models can be seen below:

```{r primary_LMM}
# A little helper function to build the models' formula
build_formula <- function(var) {
  f <- paste0(
    var, "_BMD ~ 1 + group + time + group:time + ", var, 
    "_BMD_adjust + BMI_adjust + surgery + menopause + 
    age + diabetes + thiazides + smoker + (1 | subj)"
  )
  as.formula(f)
}

LS_LMM <- lmer(formula = build_formula("LS"), data = LS_data)

# build_formula("LS") yields the following formula:
# LS_BMD ~ 1 + group + time + group:time + LS_BMD_adjust + BMI_adjust + 
#    surgery + menopause + age + diabetes + thiazides + smoker + 
#    (1 | subj)
```

After building the model, some parameters were calculated. The R^2^ was determined through the `rsquared()` function of the `piecewiseSEM` [package](https://cran.r-project.org/web/packages/piecewiseSEM/piecewiseSEM.pdf). Afther that, the `anova()` function (`stats` package) is used to compute the F tests associated with the model fixed effects and the `summary()` function (`base` package) is used to calculate the fixed and random effects estimates. Then, the estimated marginal means for the main effects and their interaction are given by the `emmeans()` function (`emmeans` [package](https://cran.r-project.org/web/packages/emmeans/emmeans.pdf)). Afterwards, the post hoc tests, without any adjustments, were done using the `pairs()` method for  `emmGrid`  objects. As the treatment effect that we were interested in was the comparison between groups at 12-months post bariatric surgery, no post hoc adjustments needed to be done. Finally, the Cohen's d was computed as effect size, using the `eff_size()` function (`emmeans` [package](https://cran.r-project.org/web/packages/emmeans/emmeans.pdf)). The code to compute all of these parameters is shown below:

```{r primary_LMM_parameters}
# R-squared
rsquared(LS_LMM)

# Fixed effects test
anova(LS_LMM, type = 3, test = "F")

# Random components and fixed effects parameters estimates
summary(LS_LMM)

# Estimated marginal means for group
group_LS_emm <- emmeans(LS_LMM, ~ group)

# Estimated marginal means for time
time_LS_emm <- emmeans(LS_LMM, ~ time)

# Estimated marginal means for group x time interaction
interaction_LS_emm  <- emmeans(LS_LMM, ~ group:time)
# Save into a data frame to build the plots
interaction_LS_emm_df <- as.data.frame(interaction_LS_emm)
write_csv(interaction_LS_emm_df, here("output", "interaction_LS_emm.csv"))

# Post hoc
ph_LS_none <- pairs(interaction_LS_emm, adjust = "none")

# Mean difference
mean_difference(ph_LS_none)
eff_size(
  interaction_LS_emm,
  sigma = sigma(LS_LMM),
  edf = 137
)

```

<br>

## Bone mineral density prediction

Afterwards, to better describe the effects of the exercise training on bone mass, some potential mechanisms by which exercise might have indirectly influenced BMD response were tested. 

For that purpose, body composition, lower limb muscle strength, and daily physical activity were analyzed. So, to test for the effect of the exercise intervention on these variables, again, linear mixed models were utilized using the same procedures as in the bone parameters linear mixed models. These analyses were done in the [05_pri_anal_model_BC_PA_strength_vars.R](https://github.com/verasls/BaSEIB_exercise_bone_health/blob/master/code/scripts/05_pri_anal_model_BC_PA_strength_vars.R) script. As only the whole body lean mass and the daily number of high-impact gravitational loads have shown to be significantly influenced by the exercise training, these two variables were included in further analyses to test if these two factors were related with BMD.

Before running the models, a new data frame was created joining the accelerometer data with the lean mass and BMD data. Also, the BMD variables, whole body lean mass and number of high-impacts were log-transformed.

```{r BMD_prediction_setup}
acc <- read_csv(
  here("data", "acc.csv"),
  col_types = cols(
    time = col_factor(1:4),
    group = col_factor(c("Control", "Exercise"))
  )
) %>% 
  select(subj, time, group, n_peaks = above_thrsh)

df <- read_data(here("data", "df.csv")) %>% 
  left_join(acc, by = c("subj", "time", "group"))

# Set contrasts of variable time to polynomial
contrasts(df$time) <- contr.poly(4)

# Transform variables
df$LS_BMD <- log(df$LS_BMD + 1)
df$TR_BMD <- log(df$TR_BMD + 1)
df$FN_BMD <- log(df$FN_BMD + 1)
df$whole_body_lean_mass <- log(df$whole_body_lean_mass + 1)
df$n_peaks <- log(df$n_peaks + 1)
```

Subsequently, separate models were run for the lumbar spine, 1/3 radius and femoral neck. Also, in each of these placements, two separate models were run to test the effects of the lean mass and number of high-impacts on the the BMD. Linear mixed-models were developed  using the `lmer()` function of the `lme4` [package](https://cran.r-project.org/web/packages/lme4/lme4.pdf). The models included the lean mass or number of high-impacts as fixed factor and the subjects and time as random factors. After the models were built, the `summary()` function of the base package was used to obtain the models coefficients and their respective p-value.

```{r BMD_prediction}
# ** LS_BMD ---------------------------------------------------------------

LS_lean_mass <- lmer(
  formula = LS_BMD ~ whole_body_lean_mass + (1 | subj) + (1 | time), 
  data = df
)

LS_n_peaks <- lmer(
  formula = LS_BMD ~ n_peaks + (1 | subj) + (1 | time), 
  data = df
)

# Random components and fixed effects parameters estimates
summary(LS_lean_mass)
summary(LS_n_peaks)
```

# Secondary analysis

Subsequently, a secondary analysis was also performed dividing the exercise group (EG) into two subgroups according to the attendance to the exercise protocol: participants with attendance rate <50% (EG<50%) and attendance rate ≥50% (EG≥50% ) in addition to the control group (CG). This analysis was conducted to determine if the effects of the exercise intervention on the selected outcomes differed according to the attendance rate. The secondary analysis was applied to the BMD variables and was done in the [07_sec_anal_model_delta_BMD_vars.R](https://github.com/verasls/BaSEIB_exercise_bone_health/blob/master/code/scripts/07_sec_anal_model_delta_BMD_vars.R) script.

Similarly to the primary analyses, a sum contrast was set to the `attend_cat` variable, which now represents the groups according to their attendance rate, and a polynomial contrast was set to the `time` variable. After that, a different data frame was created for each variable analyzed, selecting only the necessary columns from the main data frame. These variables were:

- `subj`: subject identifier;
- `attend_cat` and `time`: factors included in the analysis;
- Dependent variable as percent change from baseline (e.g., `delta_FN_BMD`);
- Covariates: Age and BMI, as well as categorical variables such as surgery type, sex, menopause, thiazide diuretics use and smoking.

As in the secondary analysis percent change from the baseline was used, no adjustment for baseline values of the dependent variables were done, since at baseline all this values were 0%.

An example of the code used to this process is shown below:

```{r setup_LMM_sec}
# Set contrasts of variable group to sum
contrasts(df$attend_cat) <- matrix(rev(contr.sum(3)), ncol = 2)
# Set contrasts of variable time to polynomial
contrasts(df$time) <- contr.poly(4)

# Select variables
LS_delta_data <- df %>% 
  dplyr::select(
    subj, time, attend_cat, delta_LS_BMD, BMI_adjust, 
    surgery, age, menopause, diabetes, thiazides, smoker
  )
```

## Linear mixed models

The treatment effect on bone parameters was also tested by linear mixed-models analysis  using the same [package](https://cran.r-project.org/web/packages/lme4/lme4.pdf) and function than the primary analysis. The models included attend_cat, time and attend_cat X time interaction as fixed factors, the covariates mentioned above and the subjects as a random factor. An example of the code to build the models can be seen below:

```{r secondary_LMM}
# A little helper function to build the models' formula
build_formula <- function(var) {
  f <- paste0(
    "delta_", var, "_BMD ~ 1 + attend_cat + time + attend_cat:time + ", 
    "BMI_adjust + surgery + menopause + 
    age + diabetes + thiazides + smoker + (1 | subj)"
  )
  as.formula(f)
}

delta_LS_LMM <- lmer(formula = build_formula("LS"), data = LS_delta_data)
# build_formula("LS") yields the following formula:
# delta_LS_BMD ~ 1 + attend_cat + time + attend_cat:time + BMI_adjust + 
#    surgery + menopause + age + diabetes + thiazides + smoker + 
#    (1 | subj)

```

Then, the same parameters (R^2^, F tests for fixed effects, fixed and random effects estimates, estimated marginal means, post hoc tests and effect sizes) were computed using the same procedures than the primary analysis. An example of the code for these computations is shown below:

```{r secondary_LMM_parameters}
# R-squared
rsquared(delta_LS_LMM)

# Fixed effects test
anova(delta_LS_LMM, type = 3, test = "F")

# Random components and fixed effects parameters estimates
summary(delta_LS_LMM)

# Estimated marginal means for group
group_delta_LS_emm <- emmeans(delta_LS_LMM, ~ attend_cat)

# Estimated marginal means for time
time_delta_LS_emm <- emmeans(delta_LS_LMM, ~ time)

# Estimated marginal means for group x time interaction
interaction_delta_LS_emm  <- emmeans(delta_LS_LMM, ~ attend_cat:time)
# Save into a data frame to build the plots
interaction_delta_LS_emm_df <- as.data.frame(interaction_delta_LS_emm)
write_csv(interaction_delta_LS_emm_df, here("output", "interaction_delta_LS_emm.csv"))

# Post hocs
ph_delta_LS_none <- pairs(interaction_delta_LS_emm, adjust = "none")
ph_delta_LS_bonf <- bonferroni(as.data.frame(ph_delta_LS_none), 3)

# Mean difference
mean_difference_delta(ph_delta_LS_none)
eff_size(
  interaction_delta_LS_emm,
  sigma = sigma(delta_LS_LMM),
  edf = 150
)
```

<br>

# Supplementary Information - Number of acceleration peaks computation

The number of acceleration peaks above the threshold of 4.9*g* for both daily physical activity and training sessions analyses were computed through a python script ([08_analyze_training_sessions.py](https://github.com/verasls/BaSEIB_exercise_bone_health/blob/master/code/scripts/08_analyze_training_sessions.py)).

First, the csv file with the raw accelerometer data was read into a `pandas` DataFrame, and each accelerometer axis was converted into a `numpy` ndarray.

```{python}
data = pd.read_csv(fullpath, skiprows=10)
# Put each axis into a ndarray
aX = data.iloc[:, 1].to_numpy()
aY = data.iloc[:, 2].to_numpy()
aZ = data.iloc[:, 3].to_numpy()
```

Then, data from each axis was filtered using a Butterworth 4th order low-pass filter with 20Hz cutoff frequency. The filter was designed using the `scipy.signal.butter()` with a second-order section output, and applied using the `scipy.signal.sosfiltfilt()` function. Subsequently, the resultant acceleration was computed.

```{python}
# Create the lowpass filter
samp_freq = 100  # sampling frequency (Hz)
N = 4  # filter order
cutoff = 20  # cut-off frequency (Hz)
fnyq = samp_freq / 2  # Nyquist frequency
Wn = cutoff / fnyq
sos = signal.butter(N, Wn, btype="low", output="sos")

aX = signal.sosfiltfilt(sos, aX)
aY = signal.sosfiltfilt(sos, aY)
aZ = signal.sosfiltfilt(sos, aZ)
# Compute resultant acceleration
aR = np.sqrt(aX ** 2 + aY ** 2 + aZ ** 2)
```

Afterwards, the acceleration peaks with a minimum height of 4.9*g* and a minimum distance of 0.4 seconds were found with the `scipy.signal.find_peaks()` function.

```{python}
# Find acceleration peaks above 4.9g
min_height = 4.9
min_dist = 0.4 * samp_freq

peaks, _ = signal.find_peaks(aR, height=min_height, distance=min_dist)
```

And then, the mean and 95% confidence interval were calculated.

```{python}
# Compute mean and confidence interval of the number of peaks
std = df["n_peaks"].std()
n = len(df.index)

mean = df["n_peaks"].mean()
lower_ci = mean - stats.t.ppf(0.975, n - 1) * (std / sqrt(n))
upper_ci = mean + stats.t.ppf(0.975, n - 1) * (std / sqrt(n))
```

# R session info

R session info with all packages loaded and their versions.

```{r session_info, eval=TRUE}
devtools::session_info()
```